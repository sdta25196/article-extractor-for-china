# 中文文章提取

## 启动

1. yarn 
2. yarn start

## 特性

* 支持对列表和分页(包括js分页)的识别与抓取
* 支持对文章详情页面的抓取
* 支持js分页的列表抓取
* 针对更多国内网站进行了优化
* 支持 HTML5 标签 ( article, section) 
* 支持GBK、GB2312等编码
* 自动将图像和链接的相对 URL 转换为绝对 URL

## 优化方案 

* 网站第一次分析了之后，给这个网站一堆标记，然后第二次分析的时候直接根据标记优化即可。标记使用参数传入即可。
* 使用 linkedom 进行实例化，如果我们认为此也为列表页面，并且分页是js渲染的分页，就改成 jsdom进行实例化。
* ？？SPA 页面需要使用无头浏览器 | jsdom 来渲染。

## 常见规则

* 视觉分析 - 模拟真实用户，分析字号、字体、颜色信息，最终确定一个区域为内容区。
* 模板配置 - 先确认模板，如果匹配到模板直接按照模板的标准抓取。
  * 规则匹配 - 属于模板的一种，例如指定规则为有time\title\author\content。此时content是正文。
* 关键字匹配 - 正文、content、title等文案或者class。
* 深度学习 - 利用深度学习库，进行训练学习。 
  * 外层要处理的问题，一样都少不了。SPA、编码、相对链接等
* 文本分行后的文本密度 或者 标签密度 进行判断。 
  * 密度判断对短文无效
* 利用分析 + 规则匹配,计算权重重新排列dom, 来获得文章
  * 权重的判定方法决定了最终文章的准确性

本项目采取 利用分析 + 规则匹配,计算权重重新排列dom, 来获得文章。

## 规则

**详情页**

获取正文标题：
  * 提取网页的 title \ .title \ #title \ h1 \ h2

正文内容a标签密度小于 5% （a的文本量 / 全部文本）

正文内容包含大量的标点符号

正文长度大于50

嵌入标签需要分别处理

不同情况处理：
* pdf：需要拿到iframe中的地址，然后拿地址中的全部html进行处理
* js渲染：延迟拿全部html
* 内容是图片：不处理
* 详情页是文件下载
  
## TODO

* ~~拆分type~~
* ~~添加encoding 处理不同的编码，gbk、GB2312支持~~
* ~~判断页面是否符合分析逻辑~~
* ~~http与https认证~~
* ~~入口代码编写~~
* jsdom + linkdom 实现对SPA抓取，并且提供足够的优化
  * jsdom 可以执行js
  * linkdom 更快
* 列表规则、分页规则
* 301 支持